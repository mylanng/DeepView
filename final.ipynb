{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10839369-2ba7-4d07-8a7d-ea8de1870d36",
   "metadata": {},
   "source": [
    "## COMPUTER VISION: A Deep Learning Model for Ultrasound Imaging in Non-Destructive Testing ##\n",
    "\n",
    "#### Author: My Lan Nguyen, Harry Hoang, Donald Nguyen \n",
    "\n",
    "**1. Introduction**\n",
    "Ultrasound-based Non Destructive Testing (NDT) is one of the highly popular \n",
    "\n",
    "In order to address the question above, we will explore two real-world datasets: training and testing from DarkVision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c63aec1-84af-40ba-a0d9-cbfed32d655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\my lan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df64e92e-492b-4e58-a07c-a752b2c550e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import gc \n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c597dd-8d2d-4bb6-abcb-1fe1c6075ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumetricToMeshModel(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=1024, num_vertices=10000):\n",
    "        super(VolumetricToMeshModel, self).__init__()\n",
    "\n",
    "        # 3D-CNN Backbone\n",
    "        self.backbone = models.Sequential([\n",
    "            layers.Conv3D(16, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv3D(32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv3D(64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            layers.Conv3D(128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.GlobalAveragePooling3D()\n",
    "        ])\n",
    "\n",
    "        # Fully connected layers for regression\n",
    "        self.fc = models.Sequential([\n",
    "            layers.Dense(latent_dim, activation=\"relu\"),\n",
    "            layers.Dense(num_vertices * 3)  # Predict x, y, z for each vertex\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.backbone(inputs)\n",
    "        x = self.fc(x)\n",
    "        return tf.reshape(x, (-1, tf.shape(x)[1] // 3, 3))  # Output shape: (batch_size, num_vertices, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccf04de9-451d-4fe6-a6f0-7400b6009d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumetricMeshDataset(tf.data.Dataset):\n",
    "    def __new__(cls, volumes_dir, meshes_dir, input_shape=(128, 128, 128), num_vertices=10000):\n",
    "        # Get file paths\n",
    "        volume_files = sorted([os.path.join(volumes_dir, f) for f in os.listdir(volumes_dir) if f.endswith('.raw')])\n",
    "        mesh_files = sorted([os.path.join(meshes_dir, f) for f in os.listdir(meshes_dir) if f.endswith('.ply')])\n",
    "\n",
    "        def preprocess(volume_path, mesh_path):\n",
    "            # Load and normalize volume\n",
    "            with open(volume_path, 'rb') as f:\n",
    "                volume = np.frombuffer(f.read(), dtype=np.uint16).reshape((768, 768, 1280))\n",
    "            volume = volume / np.max(volume)  # Normalize to [0, 1]\n",
    "            \n",
    "            # Resize volume to input_shape\n",
    "            zoom_factors = [input_shape[0] / volume.shape[0], \n",
    "                            input_shape[1] / volume.shape[1], \n",
    "                            input_shape[2] / volume.shape[2]]\n",
    "            volume = zoom(volume, zoom_factors, order=1)  # Bilinear interpolation\n",
    "            volume = tf.expand_dims(volume, axis=-1)  # Add channel dimension\n",
    "\n",
    "            # Load mesh\n",
    "            mesh = trimesh.load(mesh_path, process=False)\n",
    "            vertices = mesh.vertices.astype(np.float32)\n",
    "            vertices = vertices - np.mean(vertices, axis=0)  # Center vertices\n",
    "            vertices = vertices / np.max(np.linalg.norm(vertices, axis=1))  # Normalize to unit sphere\n",
    "\n",
    "            # Pad vertices if fewer than num_vertices\n",
    "            if vertices.shape[0] < num_vertices:\n",
    "                padding = np.zeros((num_vertices - vertices.shape[0], 3), dtype=np.float32)\n",
    "                vertices = np.vstack([vertices, padding])\n",
    "            else:\n",
    "                vertices = vertices[:num_vertices]\n",
    "\n",
    "            return volume, vertices\n",
    "\n",
    "        def generator():\n",
    "            for vol_path, mesh_path in zip(volume_files, mesh_files):\n",
    "                yield preprocess(vol_path, mesh_path)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(input_shape[0], input_shape[1], input_shape[2], 1), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(num_vertices, 3), dtype=tf.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "736c15af-3298-4054-b072-d74f30359d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 28s/step - loss: 0.1112 - mse: 0.1112 - val_loss: 0.1484 - val_mse: 0.1484\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    volumes_dir = \"training/volumes\"\n",
    "    meshes_dir = \"training/meshes\"\n",
    "\n",
    "    dataset = VolumetricMeshDataset(volumes_dir, meshes_dir)\n",
    "    train_size = int(0.8 * len(list(dataset)))\n",
    "    test_size = len(list(dataset)) - train_size\n",
    "\n",
    "    train_dataset = dataset.take(train_size).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = dataset.skip(train_size).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Create model\n",
    "    model = VolumetricToMeshModel()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                loss=tf.keras.losses.MeanSquaredError(),\n",
    "                metrics=[\"mse\"])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(train_dataset, epochs=1, validation_data=test_dataset)\n",
    "    model.save(\"source/volumetric_to_mesh_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "759159a8-d662-494b-992d-4442faf40cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamfer Distance Function\n",
    "def chamfer_distance(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Computes the Chamfer Distance between two point clouds.\n",
    "    \"\"\"\n",
    "    dists_pred_to_gt = tf.reduce_min(\n",
    "        tf.reduce_sum((tf.expand_dims(predicted, axis=1) - tf.expand_dims(ground_truth, axis=0))**2, axis=-1), axis=1\n",
    "    )\n",
    "    dists_gt_to_pred = tf.reduce_min(\n",
    "        tf.reduce_sum((tf.expand_dims(ground_truth, axis=1) - tf.expand_dims(predicted, axis=0))**2, axis=-1), axis=1\n",
    "    )\n",
    "    chamfer = tf.reduce_mean(dists_pred_to_gt) + tf.reduce_mean(dists_gt_to_pred)\n",
    "    return chamfer.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6708cc4-8e61-4145-bf55-ad6ecd88aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hausdorff Distance Function\n",
    "def hausdorff_distance(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Computes the Hausdorff Distance between two point clouds.\n",
    "    \"\"\"\n",
    "    dists_pred_to_gt = tf.reduce_min(\n",
    "        tf.reduce_sum((tf.expand_dims(predicted, axis=1) - tf.expand_dims(ground_truth, axis=0))**2, axis=-1), axis=1\n",
    "    )\n",
    "    dists_gt_to_pred = tf.reduce_min(\n",
    "        tf.reduce_sum((tf.expand_dims(ground_truth, axis=1) - tf.expand_dims(predicted, axis=0))**2, axis=-1), axis=1\n",
    "    )\n",
    "    hausdorff = max(tf.reduce_max(dists_pred_to_gt).numpy(), tf.reduce_max(dists_gt_to_pred).numpy())\n",
    "    return hausdorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770a45e3-ce7f-48cb-9641-f0873f89b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumetricToMeshModel(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=1024, num_vertices=10000, **kwargs):\n",
    "        super(VolumetricToMeshModel, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_vertices = num_vertices\n",
    "\n",
    "        self.backbone = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv3D(16, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            tf.keras.layers.Conv3D(32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            tf.keras.layers.Conv3D(64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling3D(pool_size=2, strides=2),\n",
    "\n",
    "            tf.keras.layers.Conv3D(128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.GlobalAveragePooling3D()\n",
    "        ])\n",
    "\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(latent_dim, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(num_vertices * 3)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.backbone(inputs)\n",
    "        x = self.fc(x)\n",
    "        return tf.reshape(x, (-1, tf.shape(x)[1] // 3, 3))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(VolumetricToMeshModel, self).get_config()\n",
    "        config.update({\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"num_vertices\": self.num_vertices\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e989e8f-ff0f-40b1-9f79-1ac647987250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\My Lan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\n",
    "    \"source/volumetric_to_mesh_model.keras\",\n",
    "    custom_objects={\"VolumetricToMeshModel\": VolumetricToMeshModel}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35704893-35ce-4c44-ac9d-593a3001b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data directories\n",
    "test_volumes_dir = \"testing/volumes\"\n",
    "test_meshes_dir = \"testing/meshes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f540d3d9-99e8-46c2-97e5-48f41ab9027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "def preprocess(volume_path, mesh_path):\n",
    "    with open(volume_path, 'rb') as f:\n",
    "        volume = np.frombuffer(f.read(), dtype=np.uint16).reshape((768, 768, 1280))\n",
    "    volume = volume / np.max(volume)\n",
    "    zoom_factors = [128 / volume.shape[0], 128 / volume.shape[1], 128 / volume.shape[2]]\n",
    "    volume = zoom(volume, zoom_factors, order=1)\n",
    "    volume = tf.expand_dims(volume, axis=-1)\n",
    "    mesh = trimesh.load(mesh_path, process=False)\n",
    "    vertices = mesh.vertices.astype(np.float32)\n",
    "    vertices = vertices - np.mean(vertices, axis=0)\n",
    "    vertices = vertices / np.max(np.linalg.norm(vertices, axis=1))\n",
    "    return volume, vertices\n",
    "\n",
    "def test_generator():\n",
    "    volume_files = sorted([os.path.join(test_volumes_dir, f) for f in os.listdir(test_volumes_dir) if f.endswith('.raw')])\n",
    "    mesh_files = sorted([os.path.join(test_meshes_dir, f) for f in os.listdir(test_meshes_dir) if f.endswith('.ply')])\n",
    "    for vol_path, mesh_path in zip(volume_files, mesh_files):\n",
    "        yield preprocess(vol_path, mesh_path)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    test_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(128, 128, 128, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 3), dtype=tf.float32)\n",
    "    )\n",
    ").batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f81d66-500b-47d6-b85f-56adbb18ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Average Chamfer Distance: 1.3098429441452026\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 1\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3846e4f5-4c7e-4470-bfcd-0100c9008d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "Average Chamfer Distance: 0.9633491635322571\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 2\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d11d3190-4574-48c6-b813-47b8f033b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795ms/step\n",
      "Average Chamfer Distance: 0.8123236894607544\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 3\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d62ee9-a2f4-4cca-8a0b-137d96ec2652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872ms/step\n",
      "Average Chamfer Distance: 0.44448643922805786\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 4\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49cec875-3a69-439a-b95e-e9842da0954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Average Chamfer Distance: 0.3655959367752075\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 5\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de21e99c-d6da-49ae-8e0d-982af072bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "Average Chamfer Distance: 0.45561835169792175\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 6\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a0269f2-d4b3-4d31-870e-74e28dae23d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Average Chamfer Distance: 0.46415770053863525\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 7\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c6bf2ff-a741-46aa-a7e6-fff7aa416330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Average Chamfer Distance: 0.3692156970500946\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 8\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38c2e97f-f5fe-43f5-9675-c29ddb502130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Average Chamfer Distance: 0.4162712097167969\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 9\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef36483b-a9be-4798-ac85-41729d531603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843ms/step\n",
      "Average Chamfer Distance: 0.47973278164863586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 10\n",
    "chamfer_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        chamfer_scores.append(chamfer_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_chamfer = np.mean(chamfer_scores)\n",
    "print(f\"Average Chamfer Distance: {avg_chamfer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60544e63-014f-4124-84be-ec59b9fd8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Average Hausdorff Distance: 0.9990211129188538\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 1\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "981f7b9b-863e-43da-9886-571dfbeed446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Average Hausdorff Distance: 0.9987767338752747\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 2\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91054616-eb0e-4fef-ac22-f04bf530f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Average Hausdorff Distance: 0.9989096522331238\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 3\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc300c6-b244-4ad1-aa6c-e28f28e77d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 4\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dd65ba4-6eda-4ce7-9bc2-ba83e1441249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Average Hausdorff Distance: 0.9987020492553711\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 5\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2544dfb-d429-4481-a513-f7dd3e417ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "Average Hausdorff Distance: 0.9987960457801819\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 6\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e401332f-3394-48ac-9a44-a86767fb6daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Average Hausdorff Distance: 0.9985036849975586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 7\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fd57b2c-2494-4936-8342-818816e807e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Average Hausdorff Distance: 0.9987719058990479\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 8\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3858cd9c-50c6-4bc6-83c2-d31271ca2a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Average Hausdorff Distance: 0.998645544052124\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 9\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fecf2370-ce10-47de-9502-92d528c37a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842ms/step\n",
      "Average Hausdorff Distance: 0.999098539352417\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics for file 10\n",
    "hausdorff_scores = []\n",
    "\n",
    "for volume_batch, gt_mesh_batch in test_dataset:\n",
    "    predicted_mesh_batch = model.predict(volume_batch)\n",
    "    for predicted, ground_truth in zip(predicted_mesh_batch, gt_mesh_batch):\n",
    "        hausdorff_scores.append(hausdorff_distance(predicted, ground_truth))\n",
    "\n",
    "# Calculate average scores\n",
    "avg_hausdorff = np.mean(hausdorff_scores)\n",
    "print(f\"Average Hausdorff Distance: {avg_hausdorff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18268bd7-be19-45ae-a2e1-2f2c6952ea36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
